---
title: "Week 3 Package"
author: "고경현"
date: '2021 3 23 '
output: html_document
---

# Chapter 1. 모델링을 위한 전처리

### 문제 0. 기본 세팅
```{r message=FALSE, warning=FALSE}

library(tidyverse)
library(data.table)
library(gridExtra)

# setwd("C:/Users/RRR/Desktop/PSAT/Package/Week 3")
setwd("C:/Users/고경현/Desktop/PSAT/Week3")
data = fread("data.csv", header = T)
test = fread("test.csv", header = T)

```


## [Train data 전처리 및 EDA]

### 문제 1. `'bmi'` 변수를 `numeric`자료형으로 바꾸고 `NA`값을 mean imputation
```{r message=FALSE, warning=FALSE}

#To numeric
data <- data %>% 
  mutate( bmi = as.numeric(bmi) )

#bmi mean vector(list)
mean_bmi = data %>% 
  summarise(bmi = mean(bmi, na.rm=T)) %>% 
  as.list

#imputation
data <- data %>%
  replace_na(mean_bmi)


```

### 문제 2. 문자형 변수를 factor로
```{r}

data <- data %>% 
  mutate_if(is.character, as.factor)

```

### 문제 3. `'id'`변수 제거
```{r}
data <- data %>% 
  select(-id)

```

### 문제 4. 타겟값 별로 범주형 변수의 분포 시각화 & 해석
```{r fig1, fig.height = 6, fig.width = 13, fig.align='center'}
str(data)

p_c_stroke1 = data %>% 
  select(-age,-avg_glucose_level,-bmi) %>% 
  gather(variable, value, -stroke) %>%
  filter(stroke == 1) %>% 
  ggplot(aes(y=variable, fill=value)) +
  geom_bar(position = "fill", alpha=0.5) +
  labs(title = "Stroke : 1") +
  guides(fill = guide_legend(override.aes = list(alpha = 0.5))) +
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        legend.position="bottom", legend.title = element_blank())


p_c_stroke0 = data %>% 
  select(-age,-avg_glucose_level,-bmi) %>% 
  gather(variable, value, -stroke) %>%
  filter(stroke == 0) %>% 
  ggplot(aes(y=variable, fill=value)) +
  geom_bar(position = "fill", alpha=0.5) +
  labs(title = "Stroke : 0") +
  guides(fill = guide_legend(override.aes = list(alpha = 0.5))) +
  theme_classic() +
  theme(plot.title = element_text(hjust=0.5),
        legend.position="bottom", legend.title = element_blank())


grid.arrange(p_c_stroke1, p_c_stroke0, layout_matrix = rbind(c(1,2)))


```

<p>이 플랏은 `stroke`의 여부에 따라 각각의 범주형 자료의 분포를 나타낸 플랏이다. <br>
- 뇌졸중은 아이들에게서는 거의 보이지 않는다. <br>
- 뇌졸중에 걸린 사람은 뇌졸중에 걸리지 않은 사람에 비해 심장병과 고혈압 비율이 높다. <br>
- 뇌졸중에 걸린 사람은 뇌졸중에 걸리지 않은 사람보다 흡연 경험률이 높다. <br>
- 뇌졸중에 걸린 사람과 걸리지 않은 사람의 성별 차이는 없다. <br>
- 뇌졸중에 걸린 사람은 걸리지 않은 사람에 비해 혼인율이 높다. <br>


### 문제 5. 타겟 값 별로 수치형 변수의 분포 시각화 & 해석
```{r fig2, fig.height = 7, fig.width = 10, fig.align='center'}

p_n_stroke1 <- data %>% 
  select(stroke, age, avg_glucose_level, bmi) %>%
  filter(stroke == 1) %>% 
  gather(key = "variable", value = "value", -stroke) %>%
  ggplot(aes(x=value, color = variable)) +
  geom_density() +
  labs(xlab = "variable", title="Stroke : 1") +
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5))

p_n_stroke0 <- data %>% 
  select(stroke, age, avg_glucose_level, bmi) %>%
  filter(stroke == 0) %>% 
  gather(key = "variable", value = "value", -stroke) %>%
  ggplot(aes(x=value, color = variable)) +
  geom_density() +
  labs(xlab = "variable", title="Stroke : 0") +
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5))


grid.arrange(p_n_stroke1, p_n_stroke0, layout_matrix = rbind(1,2))

```
<p> 뇌졸중은 주로 고령자에게서 나타나고, 뇌졸중에 걸린 사람은 걸리지 않은 사람에 비해 혈당 수치가 높음을 짐작할 수 있다. <br>




### 문제 6. 타겟 변수와 범주형 변수에 대한 카이스퀘어 독립성 검정
```{r message=FALSE, warning=FALSE}



# 검정할 범주형 변수만으로 구성된 데이터 저장
cat_data <- data %>% 
  select(-age, -avg_glucose_level, -bmi, -stroke) %>% 
  mutate_if(is.numeric, as.factor)

# 검정할 타겟 변수 데이터 저장
stroke_data <- data$stroke %>% as.factor

# 검정 결과 넣을 데이터 프레임 생성
test_result = data.frame(cate_var = rep("", ncol(cat_data)), chi="")
  
# 독립성 검정
for ( i in 1:ncol(cat_data)){
  
  # p-value 계산
  temp_test = chisq.test(cat_data[,..i], stroke_data)
  pval = temp_test$p.value
  
  # 결과 저장
  result = ifelse( pval > 0.05, "accept", "denied")
  test_result[i,] = c(colnames(cat_data)[i], result)
}

test_result

```


### 문제 7. 귀무가설을 기각하지 못한 변수 제거
```{r}

data = data %>% 
  select(-gender, -Residence_type)


str(data)
```

## [Test data 전처리]

### 문제 8. test셋 전처리
```{r message=FALSE, warning=FALSE}

#To numeric
test <- test %>% 
  mutate( bmi = as.numeric(bmi) )

#bmi mean vector(list)
mean_bmi = test %>% 
  summarise(bmi = mean(bmi, na.rm=T)) %>% 
  as.list

#imputation
test <- test %>%
  replace_na(mean_bmi)

#character to factor
test <- test %>% 
  mutate_if(is.character, as.factor)

#Feature Selection
test <- test %>% 
  select(-id, -gender, -Residence_type)




```

# Chapter 2. Catboost

### 문제 0. Catboost 모델의 특성 및 대표적인 파라미터
<p> 우선 **Catboost**는 범주형 변수를 많이 포함하고 있는 데이터에 강한 부스팅 모델이다. <br>
Gradient Boosting 모델 답게 정해진 파라미터에 대해 스스로 그래디언트를 감소시키는 방향으로 learning rate를 업데이트하고 최적 모델을 위해 학습한다. 따로 범주형 변수로 전처리 하는 것 없이 처리 과정에서 알아서 전처리를 진행하여 학습한다는 장점이 있으며, GPU 상에서 작업을 수행할 때는 다른 모델보다 더 빠르다는 장점 또한 있다. <br>



<p> 파라미터<br>
- `depth` : 트리의 깊이 <br>
- `iterations`: 트리의 최대 개수 <br>
- `learning_rate`: 그래디언트 부스팅의 step을 밟아가면서 최적 모델을 찾기 위한 step <br>
- `loss_fuction`: 손실 함수로, 모델을 학습시키는 과정에서 어떤 평가 지표를 기준으로 학습할 것인지 결정 <br>



### 문제 1. `logloss_cb` 데이터 프레임 생성
```{r message=FALSE, warning=FALSE}

library(catboost)
library(caret)
library(MLmetrics)

logloss_cb = expand.grid(depth = c(4,6,8), iterations = c(100,200), logloss = NA)
```


### 문제 2. 파라미터 튜닝(Grid Search 5-fold CV)
```{r}

set.seed(1234)
fold =  createFolds(data$age, k=5)

start_time = Sys.time()
for (m in 1:nrow(logloss_cb)){

  depth = logloss_cb[m,"depth"]
  iterations = logloss_cb[m,"iterations"]
  temp_logloss = NULL
  
  #하이퍼 파라미터 튜닝 설정
  params = list(
    depth = depth,
    iterations = iterations,
    random_seed = 1234,
    loss_function = "Logloss"
  )
  
  for ( k in 1:length(fold)){
    
    #train, test data 생성
    fold_test_index = fold[[k]]
    fold_test_data = data[fold_test_index,]
    fold_train_data = data[-fold_test_index,]
   
    #학습
    train_pool = catboost.load_pool(fold_train_data[,-"stroke"], label = fold_train_data$stroke, cat_features = c(2,3,4,5,8))
    test_pool = catboost.load_pool(fold_test_data[,-"stroke"], label = fold_test_data$stroke, cat_features = c(2,3,4,5,8))

    cb_model = catboost.train(train_pool, params = params)
    
    pred_stroke = catboost.predict(cb_model, test_pool)
    true_stroke = fold_test_data$stroke
    
    #logloss 계산
    pre_logloss = LogLoss(pred_stroke, true_stroke)
    temp_logloss = c(temp_logloss, pre_logloss)
  }
  
  #각 파라미터 조합별 Logloss 평균값 저장
  mean_logloss = mean(temp_logloss)
  logloss_cb[m,"logloss"] = mean_logloss
}
end_time = Sys.time()
end_time - start_time

logloss_cb
```

### 문제 3. 가장 낮은 logloss 값 행 출력
```{r}
best_params = logloss_cb %>% 
  filter(logloss==min(logloss))

best_params

```


### 문제 4. 가장 낮은 logloss 값의 파라미터로 전체 데이터 학습 및 test에 대한 logloss값 구하기
```{r}

depth = best_params$depth 
iterations = best_params$iterations

params = list(
  depth = depth,
  iterations = iterations,
  random_seed = 1234,
  loss_function  = "Logloss",
  eval_metric = "Logloss"
)

train_pool = catboost.load_pool(data[,-"stroke"], label = data$stroke, cat_features = c(2,3,4,5,8))
test_pool = catboost.load_pool(test[,-"stroke"], label = test$stroke, cat_features = c(2,3,4,5,8))


cb_model = catboost.train(train_pool, params = params)
pred_stroke = catboost.predict(cb_model, test_pool)
true_stroke = test$stroke


LogLoss(pred_stroke, true_stroke)
```

# Chapter 3. K-means Clustering

### 문제 0.
```{r message=FALSE, warning=FALSE}
library(factoextra)
library(cluster)
```


### 문제 1. 수치형 변수에 대해 `scale` 함수로 정규화
```{r}


cluster_data = data %>% 
  select(age, avg_glucose_level, bmi) %>% 
  scale(center=T, scale=T)

```


### 문제 2. `fvis_nbclust`로 시각화 및 K 값 설정
```{r fig3, fig.height = 5, fig.width = 15, fig.align='center'}

wss_method = cluster_data %>% fviz_nbclust(kmeans, method ="wss")
silhouette_method = cluster_data %>% fviz_nbclust(kmeans, method ="silhouette")


grid.arrange(wss_method, silhouette_method,layout_matrix = rbind(c(1,2)))


```
<p> 왼쪽 플랏을 보았을 때 elbow method에 의해 기울기가 급격히 변하는 지점에서 적절한 K 값을 선택할 수 있다.<br>
따라서 k가 3또는 4임을 고려할 수 있으며, 오른쪽 실루엣 플랏을 통해 적절한 k는 4임을 확인할 수 있다.


```{r include=FALSE}

set.seed(1234)
k_cluster = cluster_data %>% kmeans(3, nstart = 1, iter.max = 30)

fviz_cluster(k_cluster, data = cluster_data,
             geom = c("point", "text"),
             ellipse.type = "convex",
             ggtheme = theme_classic())


```


### 문제 3. K-means 클러스터링 및 시각화
```{r fig4, fig.height = 6, fig.width = 8, fig.align='center'}

k_cluster = cluster_data %>% kmeans(3, nstart = 1, iter.max = 30)

fviz_cluster(k_cluster, data = cluster_data,
             geom = c("point", "text"),
             ggtheme = theme_classic())


```






### 문제 4. 수치형 변수 시각화 및 해석
```{r fig5, fig.height = 7, fig.width = 14, fig.align='center'}

p1 = data %>% 
  select(age) %>%  
  mutate(cluster = k_cluster$cluster %>% as.factor) %>% 
  ggplot(aes(x=cluster, y=age)) + 
  geom_boxplot(
    alpha = 0.6,
    color=c("#845ec2", "#ffc75f", "#ff5e78"), 
    fill=c("#845ec2", "#ffc75f", "#ff5e78"),
    outlier.shape = NA) +
  stat_boxplot(geom ='errorbar', 
               color=c("#845ec2", "#ffc75f", "#ff5e78")) + 
  theme_classic()


p2 = data %>% 
  select(avg_glucose_level) %>%  
  mutate(cluster = k_cluster$cluster %>% as.factor) %>% 
  ggplot(aes(x=cluster, y=avg_glucose_level)) + 
  geom_boxplot(
    alpha = 0.6,
    color=c("#845ec2", "#ffc75f", "#ff5e78"), 
    fill=c("#845ec2", "#ffc75f", "#ff5e78"),
    outlier.shape = NA) +
  stat_boxplot(geom ='errorbar', 
               color=c("#845ec2", "#ffc75f", "#ff5e78")) + 
  theme_classic()


p3 = data %>% 
  select(bmi) %>%  
  mutate(cluster = k_cluster$cluster %>% as.factor) %>% 
  ggplot(aes(x=cluster, y=bmi)) + 
  geom_boxplot(
    alpha = 0.6,
    color=c("#845ec2", "#ffc75f", "#ff5e78"), 
    fill=c("#845ec2", "#ffc75f", "#ff5e78"),
    outlier.shape = NA) +
  stat_boxplot(geom ='errorbar', 
               color=c("#845ec2", "#ffc75f", "#ff5e78")) + 
  theme_classic()

grid.arrange(p1, p2, p3, layout_matrix=rbind(c(1,2,3)))




```

- 클러스터1,3 의 평균 연령대가 클러스터 2에 비해 높다. <br>
- 클러스터 3의 평균 혈당 수치가 다른 클러스터에 비해 높다. <br>
- 클러스터1,3의 평균 bmi가 클러스터 2에 비해 높지만 통계적으로 유의미한 차이가 있는지는 모르겠다. <br>
- 위에서 했던 EDA과정을 통해 수치형 변수만 고려했을 때, 3번 클러스터에 속한 사람들이 뇌졸중을 겪을 확률이 더 높을 것 같다. 

























